{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "composite-spray",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T00:51:48.978051Z",
     "start_time": "2021-11-01T00:51:48.974238Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# 模拟环境PLE库 PyGame-Learning-Environment\n",
    "from ple import PLE\n",
    "from ple.games.flappybird import FlappyBird\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annual-surface",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-01T00:52:14.874965Z",
     "start_time": "2021-11-01T00:51:52.911500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0, Current score: -5.0, Max score: 0\n",
      "Episodes: 1, Current score: -5.0, Max score: 0\n",
      "Episodes: 2, Current score: -5.0, Max score: 0\n",
      "Episodes: 3, Current score: -5.0, Max score: 0\n",
      "Episodes: 4, Current score: -5.0, Max score: 0\n",
      "Episodes: 5, Current score: -5.0, Max score: 0\n",
      "Episodes: 6, Current score: -5.0, Max score: 0\n",
      "Episodes: 7, Current score: -5.0, Max score: 0\n",
      "Episodes: 8, Current score: -5.0, Max score: 0\n",
      "Episodes: 9, Current score: -5.0, Max score: 0\n",
      "Episodes: 10, Current score: -5.0, Max score: 0\n",
      "Episodes: 11, Current score: -5.0, Max score: 0\n",
      "Episodes: 12, Current score: -5.0, Max score: 0\n",
      "Episodes: 13, Current score: -5.0, Max score: 0\n",
      "Episodes: 14, Current score: -5.0, Max score: 0\n",
      "Episodes: 15, Current score: -5.0, Max score: 0\n",
      "Episodes: 16, Current score: -5.0, Max score: 0\n",
      "Episodes: 17, Current score: -5.0, Max score: 0\n",
      "Episodes: 18, Current score: -5.0, Max score: 0\n",
      "Episodes: 19, Current score: -5.0, Max score: 0\n",
      "Episodes: 20, Current score: -5.0, Max score: 0\n",
      "Episodes: 21, Current score: -5.0, Max score: 0\n",
      "Episodes: 22, Current score: -5.0, Max score: 0\n",
      "Episodes: 23, Current score: -5.0, Max score: 0\n",
      "Episodes: 24, Current score: -5.0, Max score: 0\n",
      "Episodes: 25, Current score: -5.0, Max score: 0\n",
      "Episodes: 26, Current score: -5.0, Max score: 0\n",
      "Episodes: 27, Current score: -5.0, Max score: 0\n",
      "Episodes: 28, Current score: -5.0, Max score: 0\n",
      "Episodes: 29, Current score: -5.0, Max score: 0\n",
      "Episodes: 30, Current score: -5.0, Max score: 0\n",
      "Episodes: 31, Current score: -5.0, Max score: 0\n",
      "Episodes: 32, Current score: -5.0, Max score: 0\n",
      "Episodes: 33, Current score: -5.0, Max score: 0\n",
      "Episodes: 34, Current score: -5.0, Max score: 0\n",
      "Episodes: 35, Current score: -5.0, Max score: 0\n",
      "Episodes: 36, Current score: -5.0, Max score: 0\n",
      "Episodes: 37, Current score: -5.0, Max score: 0\n",
      "Episodes: 38, Current score: -5.0, Max score: 0\n",
      "Episodes: 39, Current score: -5.0, Max score: 0\n",
      "Episodes: 40, Current score: -5.0, Max score: 0\n",
      "Episodes: 41, Current score: -5.0, Max score: 0\n",
      "Episodes: 42, Current score: -5.0, Max score: 0\n",
      "Episodes: 43, Current score: -5.0, Max score: 0\n",
      "Episodes: 44, Current score: -5.0, Max score: 0\n",
      "Episodes: 45, Current score: -5.0, Max score: 0\n",
      "Episodes: 46, Current score: -5.0, Max score: 0\n",
      "Episodes: 47, Current score: -5.0, Max score: 0\n",
      "Episodes: 48, Current score: -5.0, Max score: 0\n",
      "Episodes: 49, Current score: -5.0, Max score: 0\n",
      "Episodes: 50, Current score: -5.0, Max score: 0\n",
      "Episodes: 51, Current score: -5.0, Max score: 0\n",
      "Episodes: 52, Current score: -5.0, Max score: 0\n",
      "Episodes: 53, Current score: -5.0, Max score: 0\n",
      "Episodes: 54, Current score: -5.0, Max score: 0\n",
      "Episodes: 55, Current score: -5.0, Max score: 0\n",
      "Episodes: 56, Current score: -5.0, Max score: 0\n",
      "Episodes: 57, Current score: -5.0, Max score: 0\n",
      "Episodes: 58, Current score: -5.0, Max score: 0\n",
      "Episodes: 59, Current score: -5.0, Max score: 0\n",
      "Episodes: 60, Current score: -5.0, Max score: 0\n",
      "Episodes: 61, Current score: -5.0, Max score: 0\n",
      "Episodes: 62, Current score: -5.0, Max score: 0\n",
      "Episodes: 63, Current score: -5.0, Max score: 0\n",
      "Episodes: 64, Current score: -5.0, Max score: 0\n",
      "Episodes: 65, Current score: -5.0, Max score: 0\n",
      "Episodes: 66, Current score: -5.0, Max score: 0\n",
      "Episodes: 67, Current score: -5.0, Max score: 0\n",
      "Episodes: 68, Current score: -5.0, Max score: 0\n",
      "Episodes: 69, Current score: -5.0, Max score: 0\n",
      "Episodes: 70, Current score: -5.0, Max score: 0\n",
      "Episodes: 71, Current score: -5.0, Max score: 0\n",
      "Episodes: 72, Current score: -5.0, Max score: 0\n",
      "Episodes: 73, Current score: -5.0, Max score: 0\n",
      "Episodes: 74, Current score: -5.0, Max score: 0\n",
      "Episodes: 75, Current score: -5.0, Max score: 0\n",
      "Episodes: 76, Current score: -5.0, Max score: 0\n",
      "Episodes: 77, Current score: -5.0, Max score: 0\n",
      "Episodes: 78, Current score: -5.0, Max score: 0\n",
      "Episodes: 79, Current score: -5.0, Max score: 0\n",
      "Episodes: 80, Current score: -5.0, Max score: 0\n",
      "Episodes: 81, Current score: -5.0, Max score: 0\n",
      "Episodes: 82, Current score: -5.0, Max score: 0\n",
      "Episodes: 83, Current score: -5.0, Max score: 0\n",
      "Episodes: 84, Current score: -5.0, Max score: 0\n",
      "Episodes: 85, Current score: -5.0, Max score: 0\n",
      "Episodes: 86, Current score: -5.0, Max score: 0\n",
      "Episodes: 87, Current score: -5.0, Max score: 0\n",
      "Episodes: 88, Current score: -5.0, Max score: 0\n",
      "Episodes: 89, Current score: -5.0, Max score: 0\n",
      "Episodes: 90, Current score: -5.0, Max score: 0\n",
      "Episodes: 91, Current score: -5.0, Max score: 0\n",
      "Episodes: 92, Current score: -5.0, Max score: 0\n",
      "Episodes: 93, Current score: -5.0, Max score: 0\n",
      "Episodes: 94, Current score: -5.0, Max score: 0\n",
      "Episodes: 95, Current score: -5.0, Max score: 0\n",
      "Episodes: 96, Current score: -5.0, Max score: 0\n",
      "Episodes: 97, Current score: -5.0, Max score: 0\n",
      "Episodes: 98, Current score: -5.0, Max score: 0\n",
      "Episodes: 99, Current score: -5.0, Max score: 0\n",
      "Episodes: 100, Current score: -5.0, Max score: 0\n",
      "Episodes: 101, Current score: -5.0, Max score: 0\n",
      "Episodes: 102, Current score: -5.0, Max score: 0\n",
      "Episodes: 103, Current score: -5.0, Max score: 0\n",
      "Episodes: 104, Current score: -5.0, Max score: 0\n",
      "Episodes: 105, Current score: -5.0, Max score: 0\n",
      "Episodes: 106, Current score: -5.0, Max score: 0\n",
      "Episodes: 107, Current score: -5.0, Max score: 0\n",
      "Episodes: 108, Current score: -5.0, Max score: 0\n",
      "Episodes: 109, Current score: -5.0, Max score: 0\n",
      "Episodes: 110, Current score: -5.0, Max score: 0\n",
      "Episodes: 111, Current score: -5.0, Max score: 0\n",
      "Episodes: 112, Current score: -5.0, Max score: 0\n",
      "Episodes: 113, Current score: -5.0, Max score: 0\n",
      "Episodes: 114, Current score: -5.0, Max score: 0\n",
      "Episodes: 115, Current score: -5.0, Max score: 0\n",
      "Episodes: 116, Current score: -5.0, Max score: 0\n",
      "Episodes: 117, Current score: -5.0, Max score: 0\n",
      "Episodes: 118, Current score: -5.0, Max score: 0\n",
      "Episodes: 119, Current score: -5.0, Max score: 0\n",
      "Episodes: 120, Current score: -5.0, Max score: 0\n",
      "Episodes: 121, Current score: -5.0, Max score: 0\n",
      "Episodes: 122, Current score: -5.0, Max score: 0\n",
      "Episodes: 123, Current score: -5.0, Max score: 0\n",
      "Episodes: 124, Current score: -5.0, Max score: 0\n",
      "Episodes: 125, Current score: -5.0, Max score: 0\n",
      "Episodes: 126, Current score: -5.0, Max score: 0\n",
      "Episodes: 127, Current score: -5.0, Max score: 0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nora/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 模拟环境PLE库 PyGame-Learning-Environment\n",
    "from ple import PLE\n",
    "from ple.games.flappybird import FlappyBird\n",
    "\n",
    "# 定义智能体Agent\n",
    "class Agent():\n",
    "    def __init__(self, action_space):\n",
    "        # 获得游戏支持的动作集合\n",
    "        self.action_set = action_space\n",
    "        # 创建q-table\n",
    "        self.q_table = np.zeros((6, 6, 6, 2))\n",
    "        # 学习率\n",
    "        self.alpha = 0.7\n",
    "        # 折现因子\n",
    "        self.gamma = 0.8\n",
    "        # 贪婪率\n",
    "        self.greedy = 0.8\n",
    "\n",
    "    \"\"\"\n",
    "        提取游戏state中我们需要的数据\n",
    "        输入：\n",
    "        1）state: 游戏state\n",
    "        输出：转换为category进行返回\n",
    "    \"\"\"\n",
    "    def get_state(self, state):\n",
    "        return_state = np.zeros((3,), dtype=int)\n",
    "        # x距离\n",
    "        dist_to_pipe_horz = state[\"next_pipe_dist_to_player\"]\n",
    "        # y距离\n",
    "        dist_to_pipe_bottom = state[\"player_y\"] - state[\"next_pipe_top_y\"]\n",
    "        # 小鸟的速度\n",
    "        velocity = state['player_vel']\n",
    "        # 设置小鸟速度的等级\n",
    "        if velocity < -15:\n",
    "            velocity_category = 0\n",
    "        elif velocity < -10:\n",
    "            velocity_category = 1\n",
    "        elif velocity < -5:\n",
    "            velocity_category = 2\n",
    "        elif velocity < 0:\n",
    "            velocity_category = 3\n",
    "        elif velocity < 5:\n",
    "            velocity_category = 4\n",
    "        else:\n",
    "            velocity_category = 5\n",
    "\n",
    "        # 设置小鸟高度等级\n",
    "        if dist_to_pipe_bottom < 8:  # very close\n",
    "            height_category = 0\n",
    "        elif dist_to_pipe_bottom < 20:  # close\n",
    "            height_category = 1\n",
    "        elif dist_to_pipe_bottom < 50:  # not close\n",
    "            height_category = 2\n",
    "        elif dist_to_pipe_bottom < 125:  # mid\n",
    "            height_category = 3\n",
    "        elif dist_to_pipe_bottom < 250:  # far\n",
    "            height_category = 4\n",
    "        else:\n",
    "            height_category = 5\n",
    "\n",
    "        # 设置distance等级\n",
    "        if dist_to_pipe_horz < 8:  # very close\n",
    "            dist_category = 0\n",
    "        elif dist_to_pipe_horz < 20:  # close\n",
    "            dist_category = 1\n",
    "        elif dist_to_pipe_horz < 50:  # not close\n",
    "            dist_category = 2\n",
    "        elif dist_to_pipe_horz < 125:  # mid\n",
    "            dist_category = 3\n",
    "        elif dist_to_pipe_horz < 250:  # far\n",
    "            dist_category = 4\n",
    "        else:\n",
    "            dist_category = 5\n",
    "        # 返回等级参数\n",
    "        return_state[0] = height_category\n",
    "        return_state[1] = dist_category\n",
    "        return_state[2] = velocity_category\n",
    "        return return_state\n",
    "\n",
    "    \"\"\"\n",
    "        更新QTable\n",
    "        old_state: 执行动作前的状态\n",
    "        current_action: 执行的动作\n",
    "        next_state: 执行动作后的状态\n",
    "        r: 奖励\n",
    "    \"\"\"\n",
    "    def update_q_table(self, old_state, current_action, next_state, r):\n",
    "        # 得到下一个状态的最大值\n",
    "        next_max_value = np.max(self.q_table[next_state[0], next_state[1], next_state[2]])\n",
    "        # 更新QTable\n",
    "        self.q_table[old_state[0], old_state[1], old_state[2], current_action] += \\\n",
    "            self.alpha * (r + next_max_value - self.q_table[old_state[0], old_state[1], old_state[2], current_action])\n",
    "\n",
    "    \"\"\"\n",
    "        获得最佳的动作\n",
    "        输入： \n",
    "        1） state\n",
    "        2） greedy 是否使用ϵ-贪婪法\n",
    "        输出：最佳action\n",
    "    \"\"\"\n",
    "    def get_best_action(self, state, greedy=False):\n",
    "        # 获得q值\n",
    "        jump = self.q_table[state[0], state[1], state[2], 0]\n",
    "        no_jump = self.q_table[state[0], state[1], state[2], 1]\n",
    "        # 是否执行策略\n",
    "        if greedy:\n",
    "            if np.random.rand(1) < self.greedy:\n",
    "                return np.random.choice([0, 1])\n",
    "            else:\n",
    "                if jump > no_jump:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "        else:\n",
    "            if jump > no_jump:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "    # greedy随着训练次数增加，逐渐减少\n",
    "    def update_greedy(self):\n",
    "        self.greedy *= 0.95\n",
    "\n",
    "    \"\"\"\n",
    "        执行动作\n",
    "        输入：\n",
    "        1）env: 通过env向游戏发出动作命令\n",
    "        2）action: 动作\n",
    "        输出：reward\n",
    "    \"\"\"\n",
    "    def act(self, env, action):\n",
    "        # action_set表示游戏动作集(119，None)，其中119代表跳跃\n",
    "        r = env.act(self.action_set[action])\n",
    "        if r == 0: # 没有死\n",
    "            r = 1\n",
    "        if r == 1: # 通过一个桶\n",
    "            r = 10\n",
    "        else: # game over\n",
    "            r = -1000\n",
    "        return r\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 训练次数\n",
    "    episodes = 2_0000_0000\n",
    "    # 实例化游戏对象\n",
    "    game = FlappyBird()\n",
    "    # 模拟游戏接口\n",
    "    #env = PLE(game, fps=30, display_screen=True, force_fps=False)\n",
    "    env = PLE(game, fps=30, display_screen=True, force_fps=True)\n",
    "    # 初始化\n",
    "    env.init()\n",
    "    # 实例化Agent，将动作集传参进去\n",
    "    agent = Agent(env.getActionSet())\n",
    "    max_score = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        # 重置游戏\n",
    "        env.reset_game()\n",
    "        # 获得状态\n",
    "        state = agent.get_state(game.getGameState())\n",
    "        # 新的一局，对greedy进行衰减\n",
    "        agent.update_greedy()\n",
    "        while True:\n",
    "            # 获得最佳动作\n",
    "            action = agent.get_best_action(state)\n",
    "            # 执行action，计算reward\n",
    "            reward = agent.act(env, action)\n",
    "            # 执行action之后的state\n",
    "            next_state = agent.get_state(game.getGameState())\n",
    "            # 更新q-table\n",
    "            agent.update_q_table(state, action, next_state, reward)\n",
    "            # 获得当前分数\n",
    "            current_score = env.score()\n",
    "            state = next_state\n",
    "            if env.game_over():\n",
    "                max_score = max(current_score, max_score)\n",
    "                print('Episodes: %s, Current score: %s, Max score: %s' % (episode, current_score, max_score))\n",
    "                # 保存q-table\n",
    "                if current_score > 300:\n",
    "                    np.save(\"{}_{}.npy\".format(current_score, episode), agent.q_table)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-duration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
